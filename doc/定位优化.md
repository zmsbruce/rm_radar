# 聚类优化

*2024/3/17 by zmsbruce*

## 为什么要聚类

未优化前的定位机器人使用的算法如下图所示，其首先将输入的点云转换为深度图，然后积累背景图，再将当前深度图和背景图进行相减得到前景图，最后根据先前检测结果提取前景图中的点，转化成三维坐标。对于最后一步，其将深度图中矩形框框住的所有点按深度进行排序，将头尾抛弃一部分后取所有点的均值。

![](./images/cluster/0.png)

然而，上述方法的定位会受到噪点的干扰而导致其不准确，尤其是在矩形框内包含场地边缘的噪点时。具体而言，采用的激光雷达 Livox Horizon 采用非重复扫描，其 FOV 与积分时间有关，如下图所示：

![](./images/cluster/1.png)

在当前采用的积分时间 100ms的情况下（算法使用一个队列保存最近的三张深度图，相机帧率为 30Hz），其 FOV 覆盖率不到 60%，因此会造成激光雷达扫描到车身上的点云时多时少。然而噪点却始终保持一定程度上的数量（为什么？），这就造成了使用去年的方法在矩形框包含场地边缘时，定位会受到噪点的干扰而导致其不准确。在[雷达站23赛季总结和24赛季技术展望](https://www.notion.so/hitcrt2024vision/23-24-9e7427630f8b463aa3e2d7cb09fe9b2f)中也有相应的叙述。某一次实验结果如下图所示，x 坐标为帧数，y 坐标为车身点云数量：

![](./images/cluster/2.png)

由于噪点相对于车身点的数量较低，因此**如果使用聚类算法，每次取点数最多的一类，可以在某种程度上缓解定位不准确的问题**（但不能完全解决，原因在后面会解释）。

## 聚类

聚类算法使用了 `pcl::EuclideanClusterExtraction` ，比较容易实现。具体方法为：

- 遍历深度图，将非零像素点从相机坐标系转换到雷达坐标系，并构建像素点到索引的映射；
- 对每个 BBox，遍历区域内像素点，将非零深度值的像素点转换到雷达坐标系，按聚类编号收集候选点；
- 找出候选点最多的聚类，计算其质心并转换到世界坐标系，作为最终的位置；

## 问题

仅仅依靠聚类无法完全解决噪点问题。由之前的[实验数据](./images/cluster/2.png)可知，一个车身的点的数量波动非常大，最低可能小于20个点，这就导致了矩形框框住的车身点数量有时会小于噪点数量，从而导致这种情况下定位不准确。

一种缓解办法是制定**跟踪策略**，与聚类方法一起解决噪点带来的问题，详见 [此文档](./跟踪优化.md)。该方法的缺点是**不能完全解决问题**，当噪点影响时间较长时跟踪方法便会失效。

另一种方法是将检测模型替换为**分割模型**（项目中没有，需要各位同学自行研发），可以更好地规避噪点带来的影响。这种方法的缺点是需要**非常精准的相机内外参数**，否则属于车身的点也会变少，得不偿失。
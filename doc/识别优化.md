# 识别优化

*2024/3/17 by zmsbruce*

## 为什么要优化

优化前的识别使用 TensorRT 对 YOLO 网络进行部署，然而对图像的预处理以及对检测结果的后处理却仅仅使用 CPU。前处理中图片的每个像素都是独立的，非常适合使用 CUDA 去进行优化，后处理的解码和 NMS 也可以使用 CUDA 进行优化。二者的优化能够大幅度提升处理速度。

处理速度的提升也可以让我们使用**参数量更大**的识别模型，提升识别准确率。此外，在 GPU 进行 CUDA 预处理、后处理和推理的时候，CPU 端是处于空闲状态的，这允许 CPU 进行其它一些操作（如点云处理和聚类），进一步压缩一次检测-定位的时间。流程如下图所示：

![](./images/profile/0.png)

## 优化的内容

### 前处理

为了满足输入到 YOLO 网络的需求，其需要的前处理包括：图像缩放-图像填充-归一化-通道转换（BGR->RGB）及平面化（RGBRGBRGB->RRRGGGBBB）。这些都是点对点的操作，比较容易通过 CUDA 进行功能的实现。

### 后处理

YOLOv8 网络的输出维度为 84\*8400，其中 84 由 bbox(x, y, width, height) 和 80 个类别组成。8400 为潜在的检测条目数。其后处理包括解码（从 80 个类别中选择其置信度最高的一个，并标记其置信度），和非极大值抑制（NMS），并过滤出最终的结果。其中，NMS 是比较难进行 CUDA 优化的，因为其涉及了较多的分支语句。

### Batch

对于装甲板的推理，我们使用了动态输入的 TensorRT 引擎来进行加速。因此我们需要处理多张图片并加载到网络的输入端，而不是一个一个地进行推理。如果我们能够同时处理一个 batch 的多张图片，便会产生显著的性能提升。对于具体的实现，我们使用了 CUDA Stream 让不同流中的命令并发执行，以达到性能优化的目的。

## 优化的性能

### 时间

下面的表格记录了完整推理一张图片（包括车和装甲板）的时间对比，在这里使用了 v8s 的 TensorRT 引擎进行推理，显卡为 NVIDIA GeForce RTX 3060 Ti，可以看出加速的效果是显而易见的：

| 方法 | 耗时 |
|-|:-:|
|不进行加速 | 38ms |
| 使用 CUDA 加速预处理 | 18ms |
| 使用 CUDA 加速预处理和后处理 | 9ms |

我们尝试了使用更大的模型 v8m 对 v8s 进行替换，得到其耗时如下:

| 模型 | 耗时 |
|:-:|:-:|
| v8s | 9ms |
| v8m | 20ms |
